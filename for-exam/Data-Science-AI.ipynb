{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0e9685",
   "metadata": {},
   "source": [
    "# Grading\n",
    "**Written test(individual) - 60%**\n",
    "<br>\n",
    "This written test of 40 multiple choice questions with 4 possible answers is about all the theory presented in the lectures. There will be no Python-specific questions. The test will contain multiple choice questions. The questions are designed to apply the knowledge obtained from the lectures to test your understanding.\n",
    "<br>\n",
    "\n",
    "**Project report(group) - 40%**\n",
    "<br>\n",
    "1. The actual content of the final report is 20 pages maximum. The appendix may be 20 pages maximum. The content counts from the start of your introduction to the last sentence of your last section (conclusion). \n",
    "2. Follow the guidelines in the Writing Guidelines regarding appendices, bibliography and formatting.\n",
    "3. Only use screenshots of your graphs. Make proper tables for the other components of data understanding (e.g. descriptive statistics, formulas). That is to say â€“ no screenshots of code output. \n",
    "4. If you want to show some code to provide evidence towards fulfilling the LOs, put your code in an appendix. You may either make separate entries within that appendix with the relevant code snippets, or put your code in its entirety in one appendix and refer to the relevant lines within the text. Put the code in monospace font to discern it from regular text. Use the Word plugin for code to do this. This means also not putting in screenshots of your code.\n",
    "5. You take responsibility for paraphrasing correctly (or quoting if necessary) and the ethical use of sources, including accurate source referencing following APA guidelines. Use the standard Word functionality under References for this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a234b",
   "metadata": {},
   "source": [
    "## 1. Introduction to AI\n",
    "### What do we want to achieve with AI?\n",
    "- **Now**: Narrow AI ç‹­ä¹‰äººå·¥æ™ºèƒ½\n",
    "  - Automate processes è‡ªåŠ¨åŒ–æµç¨‹\n",
    "  - Improve decision-making æ”¹è¿›å†³ç­–è¿‡ç¨‹\n",
    "  - Build productivity tools æ„å»ºæå‡æ•ˆç‡çš„å·¥å…·\n",
    "- **Later**: AGI & Superintelligence é€šç”¨äººå·¥æ™ºèƒ½å’Œè¶…äººå·¥æ™ºèƒ½\n",
    "  - Perform any task a human can èƒ½å®Œæˆä»»ä½•äººç±»èƒ½åšçš„ä»»åŠ¡\n",
    "  - Solve novel problems èƒ½è§£å†³ä»æœªé‡åˆ°çš„æ–°é—®é¢˜\n",
    "  - Show creativity and common sense å±•ç°åˆ›é€ åŠ›ä¸å¸¸è¯†æ¨ç†èƒ½åŠ›\n",
    "\n",
    "### Narrow AI (Machine Learning)\n",
    "- Works with **available data** ä½¿ç”¨**å·²æœ‰æ•°æ®**è¿›è¡Œå·¥ä½œ\n",
    "- Focused on **specific tasks** ä¸“æ³¨äº**ç‰¹å®šä»»åŠ¡**\n",
    "- Core functions: \n",
    "  - Pattern recognitionæ¨¡å¼è¯†åˆ«ï¼šRecommendation systemsæ¨èç³»ç»Ÿ, facial recognitionäººè„¸è¯†åˆ«\n",
    "  - Predictioné¢„æµ‹ï¼šmaintenanceç»´æŠ¤, demand forecastingéœ€æ±‚é¢„æµ‹\n",
    "  - Content generationå†…å®¹ç”Ÿæˆï¼šChatGPT, Copilot, DALLÂ·E\n",
    "\n",
    "### What makes Narrow AI work?\n",
    "- **Main ingredients**:\n",
    "  - Data æ•°æ®\n",
    "  - Model of the data distribution å¯¹æ•°æ®åˆ†å¸ƒçš„å»ºæ¨¡\n",
    "  - Probability & inferential statistics æ¦‚ç‡ä¸æ¨ç†ç»Ÿè®¡æ–¹æ³•\n",
    "  - Model tuning æ¨¡å‹è°ƒä¼˜\n",
    "\n",
    "### AGI (Artificial General Intelligence)\n",
    "| Narrow AI | AGI |\n",
    "|-----------|-----|\n",
    "| Task-specificç‰¹å®šä»»åŠ¡ | Cross-domainè·¨é¢†åŸŸä»»åŠ¡ |\n",
    "| Based on past dataåŸºäºå†å²æ•°æ® | Can handle noveltyèƒ½å¤„ç†æ–°æƒ…å†µ |\n",
    "| Pattern recognitionæ¨¡å¼è¯†åˆ« | Pattern interpretation & reasoningæ¨¡å¼è§£é‡Šä¸æ¨ç† |\n",
    "| Increases efficiencyæé«˜æ•ˆç‡ | Matches/exceeds human flexibilityåŒ¹æ•Œæˆ–è¶…è¶Šäººç±»çš„çµæ´»æ€§ |\n",
    "\n",
    "<br>\n",
    "\n",
    "> Adding complexity or data â‰  more intelligence å¢åŠ å¤æ‚æ€§æˆ–æ•°æ® â‰  å¢å¼ºæ™ºèƒ½\n",
    "> Intelligence requires reasoning beyond seen data æ™ºèƒ½éœ€è¦è¶…è¶Šå·²çŸ¥æ•°æ®çš„æ¨ç†èƒ½åŠ›\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Business Understanding (CRISP-DM Phase)\n",
    "### Key Questions\n",
    "- **Business Objective**: What is the organization trying to achieve? ç»„ç»‡è¯•å›¾å®ç°ä»€ä¹ˆï¼Ÿ\n",
    "- **Business Success Criteria**: When is that considered successful? ä»€ä¹ˆæƒ…å†µä¸‹è®¤ä¸ºç›®æ ‡è¾¾æˆï¼Ÿï¼ˆæ˜ç¡®KPIï¼‰\n",
    "- **Data Mining Goal**: How can data science help? æ•°æ®ç§‘å­¦å¦‚ä½•æä¾›å¸®åŠ©ï¼Ÿ\n",
    "- **Data Mining Success Criteria**: When is the data science effort successful? å¦‚ä½•åˆ¤æ–­æ•°æ®ç§‘å­¦å·¥ä½œæ˜¯å¦æˆåŠŸï¼Ÿï¼ˆæ˜ç¡®å‡†ç¡®ç‡æˆ–RMSEç­‰ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Evaluation & Baseline Models\n",
    "### Why use a baseline model?\n",
    "- Understand your data performance äº†è§£æ•°æ®çš„åŸºæœ¬è¡¨ç°\n",
    "- Identify data or modeling issues å‘ç°æ•°æ®æˆ–å»ºæ¨¡ä¸­å­˜åœ¨çš„é—®é¢˜\n",
    "- Faster iterations with simpler models ä½¿ç”¨ç®€å•æ¨¡å‹å¯ä»¥æ›´å¿«é€Ÿåœ°è¿­ä»£\n",
    "- Interpretable results for stakeholders æä¾›å¯è§£é‡Šçš„ç»“æœç»™ç›¸å…³æ–¹å‚è€ƒ\n",
    "- Provide a fair benchmark for advanced models ä¸ºé«˜çº§æ¨¡å‹æä¾›ä¸€ä¸ªå…¬å¹³çš„å¯¹æ¯”åŸºå‡†\n",
    "\n",
    "### How to define Data Mining Success Criteria?\n",
    "Use one or more of the following methods:\n",
    "- **Relative improvement over baseline ç›¸å¯¹æå‡**: \"15% improvement over mean prediction\" æ¯”å‡å€¼é¢„æµ‹æå‡ 15%\n",
    "- **Business impact as a metric ä¸šåŠ¡å½±å“æŒ‡æ ‡**: \"10% waste reduction = â‚¬500 savings/month\" å‡å°‘ 10% æµªè´¹ = æ¯æœˆèŠ‚çœ 500 æ¬§å…ƒ\n",
    "- **Industry/regulatory benchmarks è¡Œä¸šæˆ–æ³•è§„æ ‡å‡†**: \"False negatives < 5% when predicting disease\" ç–¾ç—…é¢„æµ‹ä¸­æ¼æŠ¥ç‡ < 5%\n",
    "- **Statistical significance ç»Ÿè®¡æ˜¾è‘—æ€§æ ‡å‡†**: \"Improve math scores by +6 points to be beyond Â±3 point fluctuation\" æ•°å­¦æˆç»©æé«˜ 6 åˆ†ä»¥è¶…å‡º Â±3 çš„è‡ªç„¶æ³¢åŠ¨èŒƒå›´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0fda30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Data Understanding  \n",
    "## 4.1 Preliminary Data Inspection åˆæ­¥æ•°æ®æ£€æŸ¥\n",
    "- Data types and structure (e.g., numerical, categorical) æ•°æ®ç±»å‹ä¸ç»“æ„ï¼ˆæ•°å€¼å‹ã€åˆ†ç±»å‹ç­‰ï¼‰  \n",
    "- Variable distributions (check for normality) å˜é‡åˆ†å¸ƒï¼ˆæ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒï¼‰  \n",
    "- Correlations between variables å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§  \n",
    "- Missing values ç¼ºå¤±å€¼  \n",
    "- Is the dataset suitable for modeling? What are the potential issues? æ˜¯å¦é€‚åˆå»ºæ¨¡ï¼Ÿæœ‰å“ªäº›æ½œåœ¨é—®é¢˜ï¼Ÿ  \n",
    "\n",
    "## 4.2 Distributions  \n",
    "**Purpose**: Understand the shape, skewness, and normality of variables.  \n",
    "ç›®çš„ï¼šäº†è§£å˜é‡çš„å½¢çŠ¶ã€åæ€ã€æ˜¯å¦ç¬¦åˆæ­£æ€åˆ†å¸ƒ  \n",
    "\n",
    "**Visualization methods**:  \n",
    "- Histogram ç›´æ–¹å›¾: shows frequency of numerical variables æ•°å€¼å‹å˜é‡é¢‘ç‡   \n",
    "- Bar plot æŸ±çŠ¶å›¾: compares grouped categorical values åˆ†ç»„ï¼ˆåˆ†ç±»å˜é‡ï¼‰ä¸å…¶æ•°å€¼æ¯”è¾ƒ   \n",
    "- Boxplot ç®±çº¿å›¾: shows central tendency, distribution, and outliers  é›†ä¸­è¶‹åŠ¿ã€åˆ†å¸ƒã€ç¦»ç¾¤å€¼ \n",
    "\n",
    "Statistical tests like **t-test** and **ANOVA** can be used to test for normality. å¯ä»¥ä½¿ç”¨ t æ£€éªŒã€ANOVA æ¥æŸ¥çœ‹å˜é‡æ˜¯å¦æœä»æ­£æ€åˆ†å¸ƒã€‚ <br> \n",
    "If the distribution is not normal, apply **log transformation**: suitable for right-skewed data, reduces tail length and outlier influence. å¦‚æœä¸æ˜¯ï¼Œé‡‡ç”¨å¯¹æ•°å˜æ¢ï¼ˆlog transformï¼‰ï¼šé€‚ç”¨äºå³åå˜é‡ï¼Œæ‹‰è¿‘å°¾éƒ¨ï¼Œå‡å°‘æç«¯å€¼å½±å“ã€‚<br>   \n",
    "Alternatively, consider using **non-linear models**. æˆ–è€…ä½¿ç”¨éçº¿æ€§æ¨¡å‹ã€‚  \n",
    "\n",
    "## 4.3 Outlier Detection  \n",
    "**Purpose**: Identify and handle extreme values that may bias the model. è¯†åˆ«å’Œå¤„ç†å¼‚å¸¸ç‚¹ï¼Œé¿å…å…¶å¯¹æ¨¡å‹äº§ç”Ÿä¸å½“å½±å“  \n",
    "\n",
    "**Detection methods**:  \n",
    "- **Boxplot method**:  \n",
    "  - Lower bound = Q1 - 1.5 Ã— IQR  \n",
    "  - Upper bound = Q3 + 1.5 Ã— IQR  \n",
    "- **Z-score method**:  \n",
    "  - Z = (x - mean) / standard deviation  \n",
    "  - Z < -3 or Z > 3 is considered an outlier  \n",
    "\n",
    "**Handling strategies**:  \n",
    "- Investigate cause: data entry error or device malfunction?  è°ƒæŸ¥åŸå› ï¼šæ˜¯å¦ä¸ºå½•å…¥é”™è¯¯æˆ–è®¾å¤‡å¼‚å¸¸  \n",
    "- Delete: only if clearly erroneous  åˆ é™¤ï¼šä»…é™äºæ˜ç¡®ä¸ºé”™è¯¯æ•°æ®  \n",
    "- Retain: if it's a meaningful extreme value  ä¿ç•™ï¼šå¦‚æœæ˜¯æœ‰æ„ä¹‰çš„æç«¯å€¼  \n",
    "- Important: document all changes to avoid introducing bias  é‡è¦ï¼šè®°å½•æ‰€æœ‰ä¿®æ”¹æ“ä½œï¼Œé¿å…å¼•å…¥åå·®  \n",
    "\n",
    "Outlier â‰  Error â€” always investigate the reason.  ç¦»ç¾¤å€¼ â‰  é”™è¯¯ï¼Œä¸€å®šè¦è°ƒæŸ¥åŸå›   \n",
    "\n",
    "## 4.4 Feature Scaling  \n",
    "**Purpose**: Bring variables to a similar scale for better convergence and fair influence in models. è®©å˜é‡åœ¨ç›¸ä¼¼å°ºåº¦ä¸Šï¼Œæœ‰åˆ©äºå»ºæ¨¡æ”¶æ•›ã€é¿å…æŸå˜é‡ä¸»å¯¼æ¨¡å‹  \n",
    "\n",
    "**Common scaling methods**:\n",
    "\n",
    "| Method           | Description æè¿°                                 | Suitable scenarios é€‚ç”¨åœºæ™¯                           |\n",
    "|------------------|--------------------------------------------------|------------------------------------------------------|\n",
    "| StandardScaler   | Mean = 0, Std = 1 (Z-score standardization)      | Linear models, neural networks, gradient descent     |\n",
    "|                  | å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼ˆZ-score æ ‡å‡†åŒ–ï¼‰             | çº¿æ€§æ¨¡å‹ã€ç¥ç»ç½‘ç»œã€æ¢¯åº¦ä¸‹é™ç±»ç®—æ³•                    |\n",
    "| MinMaxScaler     | Rescales values to [0, 1] range                  | Image processing, interpretable scales               |\n",
    "|                  | æ‰€æœ‰å€¼å‹ç¼©åˆ° [0,1] åŒºé—´                           | å›¾åƒå¤„ç†ã€éœ€è¦æ¢å¤ä¸šåŠ¡å•ä½çš„æ¨¡å‹                      |\n",
    "| RobustScaler     | Based on IQR, resistant to outliers              | Data with many outliers                              |\n",
    "|                  | åŸºäº IQRï¼ŒæŠ—ç¦»ç¾¤ç‚¹                                | æ•°æ®æœ‰å¤§é‡ç¦»ç¾¤å€¼æ—¶                                   |\n",
    "\n",
    "**Note**:  \n",
    "- Always scale based on **training set only** to avoid data leakage.  ç¼©æ”¾åº”ä»…åŸºäºè®­ç»ƒé›†ï¼Œä»¥é˜²æ•°æ®æ³„éœ²  \n",
    "- Do **not** scale binary variables or categorical variables.  ä¸è¦å¯¹äºŒå…ƒå˜é‡æˆ–åˆ†ç±»å˜é‡ç¼©æ”¾  \n",
    "- Scaling is **not needed** for tree-based models.  ä¸ç”¨äºæ ‘æ¨¡å‹  \n",
    "\n",
    "## 4.5 Correlation & Multicollinearity  \n",
    "**Purpose**: Understand relationships between variables and identify redundancy. ç†è§£å˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œåˆ¤æ–­æ¨¡å‹è¾“å…¥æ˜¯å¦å†—ä½™  \n",
    "\n",
    "**Correlation metrics**:\n",
    "\n",
    "| Metric           | Description                                  | Notes                                  |\n",
    "|------------------|----------------------------------------------|----------------------------------------|\n",
    "| Pearson's r      | Measures linear correlation [-1, 1] çº¿æ€§ç›¸å…³æ€§ï¼Œå€¼åœ¨ [-1, 1] ä¹‹é—´ | Most common; for continuous variables æœ€å¸¸ç”¨ï¼Œé€‚ç”¨äºæ•°å€¼å‹å˜é‡ |\n",
    "| Spearman's Ï     | Measures monotonic rank correlation å•è°ƒå…³ç³»çš„ç§©ç›¸å…³ï¼Œé€‚ç”¨äºæ’åºæ•°æ® | Captures non-linear monotonic patterns å¯æ•æ‰éçº¿æ€§å•è°ƒå…³ç³» |\n",
    "| Distance corr.   | Captures all types of dependencies è¡¡é‡æ‰€æœ‰ç±»å‹çš„ç›¸å…³æ€§ï¼ˆéçº¿æ€§ç­‰ï¼‰| Powerful but harder to interpret æ›´å¼ºä½†ä¸æ˜“è§£é‡Š |\n",
    "\n",
    "**Tip**: Always use scatter plots to visually confirm correlation.  æ€»æ˜¯å¯è§†åŒ–ï¼ˆå¦‚æ•£ç‚¹å›¾ï¼‰æ¥éªŒè¯ç›¸å…³æ€§æ˜¯å¦åˆç†ã€‚  \n",
    "\n",
    "### 4.5.1 Multicollinearity  \n",
    "**Problems**:  \n",
    "- Unstable coefficients, difficult to interpret  ç³»æ•°ä¸ç¨³å®šï¼Œè§£é‡Šå›°éš¾  \n",
    "- Poor generalization  æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™  \n",
    "\n",
    "**Detection**:  \n",
    "- Correlation heatmap  ç›¸å…³ç³»æ•°çƒ­å›¾  \n",
    "- Variance Inflation Factor (VIF) æ–¹å·®è†¨èƒ€å› å­: VIF > 5 or 10 indicates high multicollinearity\n",
    "\n",
    "**Solutions**:  \n",
    "- Drop one variable (choose less relevant or meaningful)  åˆ é™¤å…¶ä¸­ä¸€ä¸ªå˜é‡ï¼ˆé€‰æ‹©æ›´å¼±ç›¸å…³ã€å«ä¹‰æ›´å¼±è€…ï¼‰  \n",
    "- Combine variables (e.g., average or use PCA)  åˆå¹¶å˜é‡ï¼ˆä¾‹å¦‚å¹³å‡å€¼ã€ä¸»æˆåˆ†åˆ†æç­‰ï¼‰  \n",
    "- Apply regularization methods (e.g., Ridge or Lasso regression)  ä½¿ç”¨æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆå¦‚ Ridge æˆ– Lasso å›å½’ï¼‰  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02738f",
   "metadata": {},
   "source": [
    "# 5. Data Understanding & Preparation\n",
    "æœºå™¨å­¦ä¹ ç±»å‹æµç¨‹å›¾ï¼ˆMachine Learning Flowï¼‰ï¼š<br>\n",
    "**Questionï¼šIs labeled data available or can a target value be generated? æ˜¯å¦æœ‰æ ‡æ³¨æ•°æ®æˆ–å¯ä»¥ç”Ÿæˆç›®æ ‡å€¼ï¼Ÿ**  <br><br>\n",
    "\n",
    "Yes â†’ **ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰**\n",
    "- **å›å½’ï¼ˆRegressionï¼‰**: Used to predict continuous values (e.g., housing price, temperature)ç”¨äºé¢„æµ‹è¿ç»­æ•°å€¼ï¼ˆå¦‚æˆ¿ä»·ã€æ¸©åº¦ï¼‰\n",
    "  - Typesï¼šLinear Regression çº¿æ€§å›å½’ / Non-Linear Regression éçº¿æ€§å›å½’  \n",
    "- **åˆ†ç±»ï¼ˆClassificationï¼‰**: Used to predict categories (e.g., spam detection) ç”¨äºé¢„æµ‹ç±»åˆ«ï¼ˆå¦‚åƒåœ¾é‚®ä»¶è¯†åˆ«ï¼‰  \n",
    "  - Typesï¼šLinear Classification çº¿æ€§åˆ†ç±» / Non-Linear Classification éçº¿æ€§åˆ†ç±»  \n",
    "\n",
    "<br>\n",
    "\n",
    "No â†’ **æ— ç›‘ç£å­¦ä¹ ï¼ˆUnsupervised Learningï¼‰**\n",
    "- **èšç±»ï¼ˆClusteringï¼‰**ï¼šGrouping data without target values (e.g., customer segmentation) å°†æ•°æ®åˆ†ç»„ï¼Œæ— éœ€ç›®æ ‡å€¼ï¼ˆå¦‚å®¢æˆ·åˆ†ç¾¤ï¼‰  \n",
    "\n",
    "## 5.1 å»ºæ¨¡å‡è®¾ä¸ç‰¹å¾é€‰æ‹©ï¼ˆModeling Assumptions & Feature Selectionï¼‰\n",
    "å»ºæ¨¡å‡è®¾ï¼ˆModeling Assumptionsï¼‰\n",
    "- **ä»£è¡¨æ€§ï¼ˆRepresentativeï¼‰**ï¼šSamples should represent the overall population æ ·æœ¬åº”èƒ½ä»£è¡¨æ€»ä½“  \n",
    "- **ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIID: Independent and Identically Distributedï¼‰**ï¼šEach row should be independent and from the same distribution æ¯è¡Œæ•°æ®åº”ç‹¬ç«‹ï¼Œä¸”æ¥è‡ªç›¸åŒåˆ†å¸ƒ\n",
    "\n",
    "<br>\n",
    "\n",
    "è¡Œå±‚é¢ï¼ˆRow-Levelï¼‰\n",
    "- **æµ‹é‡æ°´å¹³ï¼ˆMeasurement Levelï¼‰**ï¼šTypes of variables (e.g., numerical, categorical) å˜é‡ç±»å‹ï¼ˆæ•°å€¼å‹ã€ç±»åˆ«å‹ç­‰ï¼‰  \n",
    "- **åˆ†å¸ƒï¼ˆDistributionï¼‰**ï¼šVariable distribution (e.g., skewness, outliers) å˜é‡çš„åˆ†å¸ƒå½¢æ€ï¼ˆæ˜¯å¦åæ€ã€æ˜¯å¦æœ‰å¼‚å¸¸å€¼ï¼‰  \n",
    "\n",
    "<br>\n",
    "\n",
    "åˆ—å±‚é¢ï¼ˆColumn-Levelï¼‰\n",
    "- **å˜é‡å…³ç³»ï¼ˆRelationshipsï¼‰**ï¼šCorrelations between features and with target ç‰¹å¾ä¹‹é—´ã€ç‰¹å¾ä¸ç›®æ ‡ä¹‹é—´çš„ç›¸å…³æ€§  \n",
    "\n",
    "\n",
    "## 5.2 æ•°æ®ç±»å‹è¯†åˆ«ï¼ˆData Type Recognitionï¼‰\n",
    "- **ç‹¬ç«‹æ•°æ®ï¼ˆIndependent Dataï¼‰**ï¼šObservations are unrelated (e.g., random sampling) è§‚æµ‹å€¼ä¹‹é—´æ— å…³è”ï¼ˆå¦‚éšæœºæŠ½æ ·ï¼‰  \n",
    "- **è‡ªç›¸å…³ï¼ˆAutocorrelationï¼‰**ï¼šNearby data in time or space are more similar (e.g., temperature) æ—¶é—´æˆ–ç©ºé—´ä¸Šç›¸è¿‘çš„æ•°æ®æ›´ç›¸ä¼¼ï¼ˆå¦‚æ¸©åº¦ï¼‰  \n",
    "- **ç±»å†…ç›¸å…³ï¼ˆIntraclass Correlationï¼‰**ï¼šCorrelation within groups (e.g., experimental groups) ç»„å†…æ•°æ®ç›¸å…³ï¼ˆå¦‚å®éªŒç»„ï¼‰  \n",
    "\n",
    "<br>\n",
    "åˆ¤æ–­æ–¹æ³•ï¼šCombine dataset purpose + visualization (e.g., scatterplot, autocorrelation plot) ç»“åˆæ•°æ®é›†ç›®çš„ + å¯è§†åŒ–ï¼ˆå¦‚æ•£ç‚¹å›¾ã€è‡ªç›¸å…³å›¾ï¼‰\n",
    "\n",
    "\n",
    "## 5.3 åˆ†ç®±ï¼ˆBinningï¼‰\n",
    "- **å®šä¹‰ï¼ˆDefinitionï¼‰**ï¼šConvert continuous variables into categories å°†è¿ç»­å˜é‡ç¦»æ•£åŒ–ä¸ºç±»åˆ«   \n",
    "- **ç¤ºä¾‹ï¼ˆExampleï¼‰**ï¼šWind speed is divided into é£é€Ÿåˆ†ä¸ºï¼š  \n",
    "  - \"low\" < 10 m/s  \n",
    "  - \"medium\" 10â€“15 m/s  \n",
    "  - \"high\" > 15 m/s\n",
    "- **ç”¨é€”ï¼ˆPurposeï¼‰**ï¼šSimplify model input for classification models ç®€åŒ–æ¨¡å‹è¾“å…¥ï¼Œé€‚åº”åˆ†ç±»æ¨¡å‹  \n",
    "\n",
    "## 5.4 æ»åç‰¹å¾å·¥ç¨‹ï¼ˆLag Feature Engineeringï¼‰\n",
    "- **å®šä¹‰ï¼ˆDefinitionï¼‰**ï¼šUse past observations as new features å°†è¿‡å»çš„è§‚æµ‹å€¼ä½œä¸ºæ–°ç‰¹å¾  \n",
    "  - Lag 1 = æ˜¨å¤©çš„å€¼ â†’ **Yesterday's value**  \n",
    "  - Lag 7 = ä¸Šå‘¨åŒä¸€å¤©çš„å€¼ â†’ **Same day last week**\n",
    "-  **é€‚ç”¨åœºæ™¯ï¼ˆWhen to Useï¼‰**ï¼šTime series modeling (e.g., predicting todayâ€™s temperature) æ—¶é—´åºåˆ—å»ºæ¨¡ï¼ˆå¦‚é¢„æµ‹ä»Šæ—¥æ¸©åº¦ï¼‰  \n",
    "\n",
    "## 5.5 è‡ªç›¸å…³ä¸è¶‹åŠ¿å¯è§†åŒ–ï¼ˆAutocorrelation & Trend Visualizationï¼‰\n",
    "\n",
    "### 5.5.1 è‡ªç›¸å…³ï¼ˆAutocorrelationï¼‰\n",
    "- Measures similarity between time series and its lagged version è¡¡é‡æ—¶é—´åºåˆ—ä¸å…¶æ»åç‰ˆæœ¬çš„ç›¸ä¼¼æ€§  \n",
    "- Bars outside confidence interval â†’ Significant autocorrelation exists æ¡å½¢å›¾è½åœ¨ç½®ä¿¡åŒºé—´å¤– â†’ å­˜åœ¨æ˜¾è‘—è‡ªç›¸å…³  \n",
    "  \n",
    "### 5.5.2 è¶‹åŠ¿å¯è§†åŒ–ï¼ˆTrend Visualizationï¼‰\n",
    "- **Histogram ç›´æ–¹å›¾**: Shows frequency of numerical variables å±•ç¤ºæ•°å€¼å‹å˜é‡çš„é¢‘ç‡åˆ†å¸ƒ\n",
    "- **Bar plot æŸ±çŠ¶å›¾**: Compares grouped categorical values æ¯”è¾ƒåˆ†ç±»å˜é‡ç»„çš„æ•°å€¼\n",
    "- **Box plot ç®±çº¿å›¾**: Visualizes distribution and outliers å¯è§†åŒ–åˆ†å¸ƒå½¢æ€åŠå¼‚å¸¸å€¼\n",
    "- **Scatterplot æ•£ç‚¹å›¾**: Shows relationship between two variables å±•ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´çš„å…³ç³»\n",
    "- **Line plot æŠ˜çº¿å›¾**: Suitable for time series é€‚ç”¨äºæ—¶é—´åºåˆ—åˆ†æ\n",
    "- **Heatmap çƒ­åŠ›å›¾**: Displays correlation or matrix values å±•ç¤ºç›¸å…³æ€§æˆ–çŸ©é˜µæ•°å€¼\n",
    "\n",
    "## 5.6 åˆ†ç±»å˜é‡ç¼–ç ï¼ˆCategorical Encodingï¼‰\n",
    "### 5.6.1 ç¼–ç æ–¹æ³•ï¼ˆEncoding Methodsï¼‰\n",
    "| æ–¹æ³•ï¼ˆMethodï¼‰       | è¯´æ˜ï¼ˆExplanationï¼‰     | é€‚ç”¨æ¨¡å‹ï¼ˆSuitable Modelsï¼‰ |\n",
    "|----------------------|--------------------------|------------------------------|\n",
    "| One-hot encoding     | æ¯ä¸ªç±»åˆ«ä¸€åˆ—             | éçº¿æ€§æ¨¡å‹ï¼ˆNon-linearï¼‰    |\n",
    "| Dummy encoding       | n ç±»åˆ« â†’ n-1 åˆ—          | çº¿æ€§æ¨¡å‹ï¼Œé¿å…å…±çº¿æ€§        |\n",
    "\n",
    "**Python Tools**ï¼š\n",
    "```python\n",
    "pd.get_dummies(drop_first=True)  # Dummy ç¼–ç \n",
    "sklearn.preprocessing.OneHotEncoder()  # One-hot ç¼–ç \n",
    "``` \n",
    "\n",
    "## 5.7 æ•°æ®åˆå¹¶ï¼ˆMergingï¼‰\n",
    "\n",
    "| åˆå¹¶æ–¹å¼ï¼ˆJoin Typeï¼‰ | ä¸­æ–‡è§£é‡Šï¼ˆExplanationï¼‰       | ç‰¹ç‚¹ï¼ˆFeatureï¼‰                     |\n",
    "|------------------------|-------------------------------|--------------------------------------|\n",
    "| Inner Join             | å†…è¿æ¥                        | ä»…ä¿ç•™ä¸¤è¡¨å…±æœ‰é”®ï¼ˆonly common keysï¼‰ |\n",
    "| Left Join              | å·¦è¿æ¥                        | ä¿ç•™å·¦è¡¨å…¨éƒ¨ + åŒ¹é…å³è¡¨              |\n",
    "| Right Join             | å³è¿æ¥                        | ä¿ç•™å³è¡¨å…¨éƒ¨ + åŒ¹é…å·¦è¡¨              |\n",
    "| Full Join              | å…¨è¿æ¥                        | ä¿ç•™æ‰€æœ‰è¡Œï¼Œç¼ºå¤±å¡« NaN               |\n",
    "\n",
    "## 5.8 ç¼ºå¤±å€¼å¤„ç†ï¼ˆMissing Valuesï¼‰\n",
    "\n",
    "### 5.8.1 ç¼ºå¤±ç±»å‹ï¼ˆTypes of Missingnessï¼‰\n",
    "| ç±»å‹ï¼ˆTypeï¼‰ | ä¸­æ–‡è§£é‡Šï¼ˆExplanationï¼‰          | ç¤ºä¾‹ï¼ˆExampleï¼‰                    |\n",
    "|--------------|----------------------------------|------------------------------------|\n",
    "| MCAR         | å®Œå…¨éšæœºç¼ºå¤±ï¼ˆMissing Completely at Randomï¼‰ | ç¡¬ä»¶æ•…éšœå¯¼è‡´æ•°æ®ä¸¢å¤±              |\n",
    "| MAR          | éšæœºç¼ºå¤±ï¼Œä¸å…¶ä»–å˜é‡ç›¸å…³ï¼ˆMissing At Randomï¼‰         | å¹´è½»äººä¸æ„¿æŠ¥å‘Šå±å¹•æ—¶é—´            |\n",
    "| MNAR         | ééšæœºç¼ºå¤±ï¼Œä¸ç¼ºå¤±å€¼æœ¬èº«ç›¸å…³ï¼ˆMissing Not At Randomï¼‰   | é‡åº¦å¸æ¯’è€…éšç’ä½¿ç”¨é¢‘ç‡            |\n",
    "\n",
    "### 5.8.2 ä¸ºä»€ä¹ˆé‡è¦ï¼ˆWhy It Mattersï¼‰\n",
    "- ä¼šå¼•å…¥åå·® â†’ May introduce bias  \n",
    "- é™ä½ç»Ÿè®¡æ•ˆèƒ½ â†’ Reduces statistical power  \n",
    "- å½±å“æ¨¡å‹è®­ç»ƒ â†’ Affects model training\n",
    "\n",
    "### 5.8.3 ç¼ºå¤±å€¼å¡«è¡¥æ–¹æ³•ï¼ˆImputation Methodsï¼‰\n",
    "| æ–¹æ³•ï¼ˆMethodï¼‰        | é€‚ç”¨æƒ…å†µï¼ˆWhen to Useï¼‰ | ä¼˜ç‚¹ï¼ˆAdvantagesï¼‰ | ç¼ºç‚¹ï¼ˆDisadvantagesï¼‰           |\n",
    "|------------------------|--------------------------|--------------------|----------------------------------|\n",
    "| åˆ é™¤ï¼ˆdropnaï¼‰         | MCAR                     | ç®€å•               | ä¸¢å¤±ä¿¡æ¯ï¼ˆInformation lossï¼‰    |\n",
    "| å‡å€¼/ä¸­ä½æ•°å¡«å……       | MCAR/MAR                 | å¿«é€Ÿ               | æ‰­æ›²åˆ†å¸ƒï¼ˆDistorts distributionï¼‰|\n",
    "| å‰å‘/åå‘å¡«å……         | æ—¶é—´åºåˆ—                 | ä¿æŒè¶‹åŠ¿           | ä¸é€‚ç”¨äºçªå˜                     |\n",
    "| çƒ­/å†·å¡ç‰‡å¡«å……         | æœ‰ç›¸ä¼¼å€¼å¯å€Ÿç”¨           | ä¿æŒåˆ†å¸ƒ           | å¯èƒ½å¼•å…¥åå·®                    |\n",
    "| æ’å€¼ï¼ˆinterpolationï¼‰ | æœ‰è¶‹åŠ¿å¯æ¨æ–­             | å¤šç§æ–¹æ³•           | ä¸é€‚ç”¨äºè·³è·ƒæ•°æ®                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25384ef6",
   "metadata": {},
   "source": [
    "# 6. å›å½’æ¨¡å‹ Regression Models\n",
    "\n",
    "CRISP-DM æ˜¯ä¸€ä¸ªæ•°æ®ç§‘å­¦é¡¹ç›®çš„æ¡†æ¶ï¼ŒåŒ…å«å…­ä¸ªé˜¶æ®µï¼šä¸šåŠ¡ç†è§£ã€æ•°æ®ç†è§£ã€æ•°æ®å‡†å¤‡ã€å»ºæ¨¡ã€è¯„ä¼°å’Œéƒ¨ç½²ã€‚ (CRISP-DM is a framework for data science projects, including six phases: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, and Deployment.)\n",
    "- æ•°æ®ç†è§£ï¼šæ¢ç´¢å’Œæ€»ç»“æ•°æ®ï¼ˆä¾‹å¦‚æ£€æŸ¥æ¨¡å¼ã€å¼‚å¸¸å€¼æˆ–ç›¸å…³æ€§ï¼‰ï¼Œä¸ºå»ºæ¨¡åšå‡†å¤‡ã€‚ (Data Understanding: Explore and summarize data (e.g., check patterns, outliers, or correlations) to prepare for modeling.) æ•°æ®ç†è§£é€šè¿‡æ­ç¤ºæ¨¡å¼ï¼ˆä¾‹å¦‚çº¿æ€§æˆ–éçº¿æ€§å…³ç³»ï¼‰å¸®åŠ©é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚ (Data understanding helps select appropriate models by revealing patterns, such as linear or non-linear relationships.)\n",
    "- å»ºæ¨¡ï¼šåœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ éœ€è¦ï¼š (Modeling: In this phase, you need:)\n",
    "  - æ‹†åˆ†æ•°æ®ï¼šå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå»ºç«‹æµ‹è¯•è®¾è®¡ã€‚ (Split data: Divide data into training and test sets to establish a test design.) æ‹†åˆ†æ•°æ®ç¡®ä¿æ¨¡å‹åœ¨æœªè§è¿‡çš„æ•°æ®ä¸Šæµ‹è¯•ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯ã€‚ (Splitting data ensures the model is tested on unseen data, simulating real-world scenarios.)\n",
    "  - æ„å»ºå’Œè®­ç»ƒæ¨¡å‹ï¼šç”¨è®­ç»ƒé›†è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ã€‚ (Build and train models: Train the model with the training set and make predictions on the test set.)\n",
    "\n",
    "ç›‘ç£å­¦ä¹ åŸºäºè¾“å…¥ï¼ˆç‰¹å¾ï¼‰å’Œè¾“å‡ºï¼ˆç›®æ ‡ï¼‰æ•°æ®å¼€å‘é¢„æµ‹æ¨¡å‹ï¼Œåˆ†ä¸ºåˆ†ç±»å’Œå›å½’ä¸¤å¤§ä»»åŠ¡ã€‚ (Supervised learning develops predictive models based on input (features) and output (target) data, divided into two main tasks: classification and regression.)\n",
    "- ç›‘ç£å­¦ä¹ ï¼šæ¨¡å‹ä»å·²çŸ¥æ­£ç¡®ç­”æ¡ˆï¼ˆç›®æ ‡ï¼‰çš„æ•°æ®ä¸­å­¦ä¹ ã€‚ (Supervised learning: The model learns from data with known correct answers (target).) ä¾‹å¦‚ï¼Œæœ‰æˆ¿å±‹é¢ç§¯ï¼ˆè¾“å…¥ï¼‰å’Œä»·æ ¼ï¼ˆè¾“å‡ºï¼‰çš„æ•°æ®ï¼Œæ¨¡å‹å­¦ä¹ å¦‚ä½•æ ¹æ®é¢ç§¯é¢„æµ‹ä»·æ ¼ã€‚ (For example, with data on house size (input) and price (output), the model learns to predict price based on size.)\n",
    "- åˆ†ç±»ï¼šé¢„æµ‹ç±»åˆ«ï¼ˆä¾‹å¦‚â€œä¼šä¸‹é›¨å—ï¼Ÿâ€â†’ æ˜¯/å¦ï¼‰ã€‚ (Classification: Predict categories (e.g., â€œWill it rain?â€ â†’ Yes/No).)\n",
    "- å›å½’ï¼šé¢„æµ‹æ•°å€¼ï¼ˆä¾‹å¦‚â€œæˆ¿ä»·æ˜¯å¤šå°‘ï¼Ÿâ€â†’ 30ä¸‡ï¼‰ã€‚ (Regression: Predict numerical values (e.g., â€œWhat is the house price?â€ â†’ 300,000).) å›å½’ç”¨äºé¢„æµ‹æ•°å€¼ç»“æœï¼ˆå¦‚é”€å”®é¢æˆ–æ¸©åº¦ï¼‰ã€‚ (Regression is used to predict numerical outcomes, such as sales or temperature.)\n",
    "\n",
    "æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒæµç¨‹ï¼š (Core process of model training:)\n",
    "- Xï¼ˆç‰¹å¾/è‡ªå˜é‡ï¼‰ï¼šæ¨¡å‹è¾“å…¥ï¼Œæ˜¯æˆ‘ä»¬å·²çŸ¥çš„ã€‚ (X (features/independent variables): Model inputs, which are known to us.) ä¾‹å¦‚æ•°æ®æ¡†ä¸­çš„åˆ—ï¼Œå¦‚æˆ¿å±‹é¢ç§¯ã€å¹´é¾„ã€‚ (For example, columns in a dataset, such as house size or age.)\n",
    "- yï¼ˆç›®æ ‡å˜é‡/å› å˜é‡ï¼‰ï¼šæ¨¡å‹è¾“å‡ºï¼Œæ˜¯æˆ‘ä»¬è¦é¢„æµ‹çš„ï¼Œå¦‚æˆ¿ä»·ã€‚ (y (target variable/dependent variable): Model output, what we want to predict, such as house price.)\n",
    "\n",
    "è®­ç»ƒè¿‡ç¨‹ï¼š (Training process:)\n",
    "- åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚éšæœºæƒé‡ï¼‰ã€‚ (Initialize the model (e.g., with random weights).)\n",
    "- é¢„æµ‹yå€¼ã€‚ (Predict y values.)\n",
    "- è®¡ç®—è¯¯å·®ï¼ˆé¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®ï¼‰ã€‚ (Calculate error (difference between predicted and actual values).)\n",
    "- æ›´æ–°æ¨¡å‹å‚æ•°ï¼ˆä¾‹å¦‚è°ƒæ•´æƒé‡ï¼‰ã€‚ (Update model parameters (e.g., adjust weights).)\n",
    "- é‡å¤ç›´åˆ°æ»¡è¶³åœæ­¢æ¡ä»¶ï¼ˆå¦‚è¯¯å·®è¶³å¤Ÿå°ï¼‰ã€‚ (Repeat until a stopping condition is met (e.g., error is small enough).)\n",
    "\n",
    "å»ºæ¨¡å‰çš„å‡è®¾ï¼š<br> (Assumptions before modeling:)<br>\n",
    "åœ¨è¿›è¡Œå»ºæ¨¡å‰ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥æ•°æ®æ˜¯å¦æ»¡è¶³ä¸€äº›æ¡ä»¶ï¼ˆå‡è®¾ï¼‰ï¼Œä¾‹å¦‚ï¼Œçº¿æ€§å›å½’å‡è®¾ç‰¹å¾å’Œç›®æ ‡å‘ˆç›´çº¿å…³ç³»ã€‚ (Before modeling, we need to check if the data meets certain conditions (assumptions), for example, linear regression assumes a linear relationship between features and target.) è¿™äº›æ¡ä»¶çš„å­˜åœ¨æ˜¯ä¸ºäº†ç¡®ä¿æˆ‘ä»¬å¾—åˆ°çš„ç»“æœæ˜¯å¯ä¿¡çš„ã€èƒ½è¢«æ­£ç¡®è§£é‡Šçš„ã€‚ (These conditions ensure the results are reliable and interpretable.)<br>\n",
    "å¦‚æœè¿™äº›å‡è®¾è¢«è¿èƒŒï¼Œæˆ‘ä»¬å°±å¯èƒ½éœ€è¦ï¼šæ”¹ç”¨å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…åœ¨æŠ¥å‘Šä¸­è¯´æ˜ç»“æœå¯èƒ½ä¼šå¤±çœŸã€‚ (If these assumptions are violated, we may need to switch to other models or explain in the report that results may be distorted.)\n",
    "\n",
    "## 6.1 çº¿æ€§å›å½’ Linear Regression\n",
    "çº¿æ€§å›å½’å»ºæ¨¡çš„ç›®çš„æ˜¯æ‰¾åˆ°ä¸æ‰€æœ‰æ•°æ®ç‚¹æœ€æ¥è¿‘çš„ç›´çº¿ï¼Œå°½é‡å‡å°‘è¯¯å·®ï¼ˆé¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®è·ï¼‰ã€‚ (The purpose of linear regression is to find the line closest to all data points, minimizing errors (difference between predicted and actual values).) çº¿æ€§å›å½’ç®€å•ä¸”æ˜“äºè§£é‡Šï¼Œæ˜¯æ•°å€¼é¢„æµ‹çš„è‰¯å¥½èµ·ç‚¹ã€‚ (Linear regression is simple and interpretable, a good starting point for numerical predictions.)<br>\n",
    "æ ‡å‡†çš„ä¸€å…ƒçº¿æ€§å›å½’å…¬å¼æ˜¯ï¼šy = Î²0 + Î²1 * X + Ïµ (The standard simple linear regression formula is: y = Î²0 + Î²1 * X + Ïµ)\n",
    "- ğ‘¦ï¼šé¢„æµ‹å€¼ã€‚ (Predicted value.)\n",
    "- Î²0ï¼šæˆªè·ï¼Œå³å½“X=0æ—¶çš„yå€¼ï¼ˆåŸºçº¿ï¼‰ã€‚ (Intercept, the value of y when X=0 (baseline).)\n",
    "- Î²1ï¼šæ–œç‡ï¼Œè¡¨ç¤ºXæ¯å¢åŠ 1ï¼Œyä¼šå¢åŠ å¤šå°‘ã€‚ (Slope, indicating how much y increases per unit of X.)\n",
    "- Xï¼šè‡ªå˜é‡ã€‚ (Independent variable.)\n",
    "- Ïµï¼šè¯¯å·®é¡¹ï¼ˆçœŸå®å€¼ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ï¼‰ã€‚ (Error term (difference between actual and predicted values).)\n",
    "\n",
    "å¤šå…ƒçº¿æ€§å›å½’ä¸ç³»æ•°è§£é‡Šï¼šé¢å¯¹å¤šä¸ªç‰¹å¾æ—¶çš„æƒ…å†µï¼Œå¦‚æœæœ‰å¤šä¸ªè‡ªå˜é‡ï¼ˆç‰¹å¾ï¼‰ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå›å½’å¹³é¢è€Œä¸æ˜¯å›å½’çº¿ã€‚ (Multiple linear regression and coefficient interpretation: In cases with multiple features, if there are multiple independent variables (features), we get a regression plane instead of a line.) ç³»æ•°è¡¨ç¤ºè¯¥ç‰¹å¾å¯¹ç›®æ ‡å˜é‡çš„å½±å“ç¨‹åº¦ã€‚ (Coefficients indicate the impact of each feature on the target variable.) å¦‚æœæˆ‘ä»¬å°†æ¯ä¸ªç‰¹å¾çš„ç³»æ•°ä¹˜ä»¥å®ƒçš„æ ‡å‡†å·®ï¼Œå¯ä»¥è¡¡é‡å®ƒå¯¹yçš„â€œå®é™…å½±å“åŠ›â€ã€‚ (If we multiply each featureâ€™s coefficient by its standard deviation, we can measure its â€œactual impactâ€ on y.)\n",
    "\n",
    "çº¿æ€§æ¨¡å‹çš„ç±»å‹ï¼ˆLasso ä¸ Ridgeï¼‰ï¼š (Types of linear models (Lasso and Ridge):)\n",
    "1. Lasso å›å½’ï¼ˆL1æ­£åˆ™åŒ–ï¼‰ï¼šä¼šå°†ä¸é‡è¦çš„ç‰¹å¾ç³»æ•°å‹ç¼©ä¸º0 â†’ å®ç°ç‰¹å¾é€‰æ‹©ï¼Œé€‚ç”¨äºå°‘é‡ç‰¹å¾æœ‰ç”¨çš„æƒ…å†µã€‚ (Lasso regression (L1 regularization): Compresses coefficients of unimportant features to 0 â†’ achieves feature selection, suitable when only a few features are useful.)\n",
    "2. Ridge å›å½’ï¼ˆL2æ­£åˆ™åŒ–ï¼‰ï¼šä¸ä¼šå°†ç³»æ•°å‹ä¸º0ï¼Œä½†ä¼šç¼©å°å®ƒä»¬ â†’ é™ä½è¿‡æ‹Ÿåˆé£é™©ï¼Œé€‚ç”¨äºæ‰€æœ‰ç‰¹å¾å¯èƒ½éƒ½æœ‰ç”¨çš„æƒ…å†µã€‚ (Ridge regression (L2 regularization): Does not compress coefficients to 0 but shrinks them â†’ reduces overfitting risk, suitable when all features may be useful.)\n",
    "\n",
    "### 6.1.1 çº¿æ€§å›å½’å‡è®¾ Linear Regression Assumptions\n",
    "çº¿æ€§å›å½’ä¾èµ–å››ä¸ªå…³é”®å‡è®¾ï¼š (Linear regression relies on four key assumptions:)\n",
    "\n",
    "| å‡è®¾ Assumption | å«ä¹‰ Meaning | æ£€æŸ¥æ–¹å¼ Check Method | ä¿®å¤æ–¹å¼ Fix Method | ä¸¾ä¾‹ Example |\n",
    "|----------------|-------------|---------------|---------------|---------------|\n",
    "| **çº¿æ€§å…³ç³» Linear relationship** | æ¯ä¸ªç‰¹å¾ä¸ç›®æ ‡çš„å…³ç³»å¿…é¡»æ˜¯ç›´çº¿ã€‚ (Each featureâ€™s relationship with the target must be linear.) | æ•£ç‚¹å›¾ / Pairplot (Scatter plot / Pairplot) | ä½¿ç”¨éçº¿æ€§æ¨¡å‹ (Use non-linear models) | å¦‚æœé”€å”®é¢éšå¹¿å‘Šæ”¯å‡ºç¨³å®šå¢åŠ ï¼Œæ˜¯çº¿æ€§å…³ç³»ï¼›å¦‚æœè¾¾åˆ°æŸç‚¹åè¶‹å¹³ï¼Œåˆ™æ˜¯éçº¿æ€§ã€‚ (If sales increase steadily with ad spend, itâ€™s linear; if it plateaus after a point, itâ€™s non-linear.) |\n",
    "| **è§‚å¯Ÿå€¼ç‹¬ç«‹ Independence of observations** | æ¯ä¸ªæ•°æ®ç‚¹ï¼ˆè¡Œï¼‰ä¸åº”å½±å“å…¶ä»–ç‚¹ã€‚ (Each data point (row) should not affect others.) æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¾‹å¦‚æ¯æ—¥é”€å”®é¢ï¼‰å¸¸è¿åæ­¤å‡è®¾ï¼Œå› ä¸ºä»Šå¤©çš„æ•°æ®å¯èƒ½ä¾èµ–æ˜¨å¤©ã€‚ (Time series data (e.g., daily sales) often violates this because todayâ€™s data may depend on yesterdayâ€™s.) | Durbin-Watsonæ£€éªŒ (Durbin-Watson test) | åŠ å…¥æ»åå˜é‡ / æ”¹ç”¨æ—¶åºæ¨¡å‹ (Add lag variables / Use time series models) |  |\n",
    "| **æ— å¤šé‡å…±çº¿æ€§ No multicollinearity** | ç‰¹å¾ä¹‹é—´ä¸å¼ºç›¸å…³ï¼ˆä¾‹å¦‚æˆ¿å±‹é¢ç§¯å’Œæˆ¿é—´æ•°é«˜åº¦ç›¸å…³ï¼Œæä¾›ç±»ä¼¼ä¿¡æ¯ï¼‰ã€‚ (Features should not be strongly correlated (e.g., house size and number of rooms are highly correlated, providing similar information).) | çƒ­åŠ›å›¾ã€VIF > 5 (Heatmap, VIF > 5) | åˆ é™¤æˆ–åˆå¹¶å˜é‡ (Remove or combine variables) | é¢„æµ‹æ±½è½¦ä»·æ ¼æ—¶ï¼Œå¦‚æœå¼•æ“å¤§å°å’Œé©¬åŠ›é«˜åº¦ç›¸å…³ï¼Œé‚£å°±è¦ç§»é™¤ä¸€ä¸ªæˆ–åˆå¹¶ã€‚ (When predicting car prices, if engine size and horsepower are highly correlated, remove one or combine them.) |\n",
    "| **æ®‹å·®æ­£æ€åˆ†å¸ƒ/åŒæ–¹å·®æ€§ Normality of residuals/Homoscedasticity** | è¯¯å·®åº”å¹³å‡åˆ†å¸ƒä¸”æ–¹å·®ä¸€è‡´ã€‚ (Errors should be evenly distributed with constant variance.) æ®‹å·®æŒ‡é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å·®ï¼›æ­£æ€åˆ†å¸ƒæŒ‡æ®‹å·®åº”å‘ˆé’Ÿå½¢ï¼ˆæ­£æ€ï¼‰åˆ†å¸ƒï¼›åŒæ–¹å·®æ€§æŒ‡æ®‹å·®çš„æ–¹å·®åœ¨æ‰€æœ‰é¢„æµ‹å€¼ä¸­åº”æ’å®šï¼ˆæ®‹å·®å›¾æ— æ¼æ–—å½¢ï¼‰ã€‚ (Residuals are the difference between predicted and actual values; normality means residuals should follow a bell-shaped (normal) distribution; homoscedasticity means residualsâ€™ variance should be constant across all predicted values (no funnel shape in residual plots).) | æ®‹å·®å›¾ã€Q-Qå›¾ (Residual plots, Q-Q plots) | å˜é‡è½¬æ¢ï¼ˆä¾‹å¦‚å¯¹yå–å¯¹æ•°ï¼‰æˆ–æ¢æ¨¡å‹ (Variable transformation (e.g., log-transform y) or change models) | æ¯”å¦‚ï¼Œå¦‚æœæ®‹å·®å›¾å‘ˆæ¼æ–—å½¢ï¼ˆé¢„æµ‹å€¼é«˜æ—¶è¯¯å·®æ›´å¤§ï¼‰ï¼Œè¯´æ˜æ–¹å·®ä¸ä¸€è‡´ï¼Œéœ€æ¢æ¨¡å‹ã€‚ (For example, if residual plots show a funnel shape (larger errors at higher predictions), it indicates non-constant variance, requiring a different model.) |\n",
    "\n",
    "## 6.2 éçº¿æ€§æ¨¡å‹ Non-linear Models\n",
    "å¦‚æœæ•°æ®å‘ˆéçº¿æ€§å…³ç³»ï¼ˆå½“Xå’Œyä¸å‘ˆç›´çº¿å…³ç³»ï¼Œä¾‹å¦‚é”€å”®é¢æœ€åˆå¿«é€Ÿå¢åŠ ä¹‹ååˆå‡ç¼“ï¼‰ï¼Œä½¿ç”¨ä»¥ä¸‹æ¨¡å‹ï¼š (If the data shows a non-linear relationship (when X and y are not linearly related, e.g., sales increase rapidly at first then slow down), use the following models:)\n",
    "- å†³ç­–æ ‘å›å½’ï¼šç®€å•ä½†å®¹æ˜“è¿‡æ‹Ÿåˆã€‚ (Decision tree regression: Simple but prone to overfitting.)\n",
    "- éšæœºæ£®æ—å›å½’ï¼šç”¨å¤šä¸ªæ ‘å¹³å‡é¢„æµ‹ï¼Œæ›´ç¨³å¥ã€‚ (Random forest regression: Averages predictions from multiple trees, more robust.) å¹³å‡é¢„æµ‹ï¼Œå‡å°‘è¿‡æ‹Ÿåˆã€‚ (Average predictions, reducing overfitting.)\n",
    "- æ¢¯åº¦æå‡å›å½’ï¼šé€æ£µæ ‘æ„å»ºï¼Œçº æ­£å‰ä¸€æ£µæ ‘çš„é”™è¯¯ï¼Œé€šå¸¸æ›´å‡†ç¡®ã€‚ (Gradient boosting regression: Builds trees sequentially, correcting errors of previous trees, usually more accurate.)\n",
    "\n",
    "## 6.3 æ—¶é—´åºåˆ—æ¨¡å‹ Time Series Models\n",
    "çº¿æ€§å›å½’ç­‰æ ‡å‡†æ¨¡å‹ä¸é€‚åˆæ—¶é—´åºåˆ—ï¼Œå› ä¸ºæ•°æ®ç‚¹æœ‰æ—¶é—´ä¾èµ–æ€§ï¼ˆè‡ªç›¸å…³ï¼‰ã€‚ (Standard models like linear regression are unsuitable for time series due to temporal dependencies (autocorrelation).) æ—¶é—´åºåˆ—ï¼ˆæ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼‰æ•°æ®ï¼ˆä¾‹å¦‚æ¯æ—¥è‚¡ä»·ã€å¤©æ°”ã€é”€å”®é¢ï¼‰éœ€è¦ç‰¹æ®Šæ¨¡å‹ï¼š (Time series (data ordered by time) data (e.g., daily stock prices, weather, sales) requires special models:)\n",
    "- ç§»åŠ¨å¹³å‡ï¼šå–å‰å‡ æœŸå€¼çš„å¹³å‡å€¼é¢„æµ‹ä¸‹ä¸€æœŸï¼ˆä¾‹å¦‚ç”¨å‰7å¤©é”€å”®é¢å¹³å‡å€¼é¢„æµ‹æ˜å¤©ï¼‰ï¼Œå¯ä»¥å¹³æ»‘å™ªå£°ã€‚ (Moving average: Takes the average of previous periods to predict the next (e.g., use the average of the last 7 daysâ€™ sales to predict tomorrow), smoothing noise.)\n",
    "- (S)ARIMA(X)ï¼šé«˜çº§æ¨¡å‹ï¼Œé¢„æµ‹è¶‹åŠ¿å’Œå­£èŠ‚æ€§ã€‚ (Advanced model for forecasting trends and seasonality.)\n",
    "\n",
    "## 6.4 å›å½’æŒ‡æ ‡ Regression Metrics\n",
    "\n",
    "| æŒ‡æ ‡ Metric | è¯´æ˜ Description | é€‚ç”¨åœºæ™¯ Applicable Scenarios |\n",
    "|--------------------------|---------------------|---------------------------|\n",
    "| **MAE å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute Error** | é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¹³å‡ç»å¯¹å·®ï¼Œå•ä½ä¸ç›®æ ‡ç›¸åŒã€‚ (The average absolute difference between predicted and actual values, in the same units as the target.) æ‰€æœ‰é”™è¯¯å¹³ç­‰å¯¹å¾…ã€‚ (All errors are treated equally.) å¦‚æœæˆ¿ä»·é¢„æµ‹çš„MAE=1ä¸‡ï¼Œè¯´æ˜å¹³å‡è¯¯å·®ä¸º1ä¸‡å…ƒã€‚ (If MAE=10,000 for house prices, the average error is 10,000.) | æ—¥å¸¸ä½¿ç”¨ (Everyday use) |\n",
    "| **MSE / RMSE å‡æ–¹è¯¯å·® / æ ¹å‡æ–¹è¯¯å·® Mean Squared Error / Root Mean Squared Error** | å…ˆå¹³æ–¹è¯¯å·®å†å¹³å‡ç„¶åå¼€æ ¹ï¼Œæ”¾å¤§å¤§è¯¯å·®çš„å½±å“ï¼Œé€‚åˆå¤§è¯¯å·®ä»£ä»·é«˜çš„æƒ…å†µï¼ˆä¾‹å¦‚è¯ç‰©å‰‚é‡é¢„æµ‹ï¼‰ã€‚ (Squares errors, averages them, then takes the square root, emphasizing larger errors, suitable for cases where large errors are costly (e.g., drug dosage prediction).) | é«˜é£é™©é¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰ (High-risk areas (e.g., healthcare)) |\n",
    "| **RÂ² å†³å®šç³»æ•° Coefficient of Determination** | æ¨¡å‹è§£é‡Šçš„æ–¹å·®æ¯”ä¾‹ï¼Œä»…é€‚ç”¨äº**çº¿æ€§æ¨¡å‹**ã€‚ (The proportion of variance explained by the model, only reliable for **linear models**.) | è¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆæ³¨æ„ï¼šéçº¿æ€§æ¨¡å‹ä¸­RÂ²å¯èƒ½æ— æ„ä¹‰æˆ–ä¸ºè´Ÿï¼‰ (Closer to 1 is better (note: RÂ² may be meaningless or negative in non-linear models)) |\n",
    "\n",
    "## 6.5 æ•°æ®åˆ‡åˆ†ä¸éªŒè¯ Data Splitting and Validationï¼ˆTrain-Test Split & Cross-Validationï¼‰\n",
    "\n",
    "**è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ† Train-Test Split**ï¼š\n",
    "- è®­ç»ƒé›†ï¼šç”¨äºæ‹Ÿåˆæ¨¡å‹ï¼ˆä¾‹å¦‚80%ï¼‰ã€‚ (Training set: Used to fit the model (e.g., 80%).)\n",
    "- æµ‹è¯•é›†ï¼šç”¨äºè¯„ä¼°æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šçš„è¡¨ç°ï¼ˆä¾‹å¦‚20%ï¼‰ã€‚ (Test set: Used to evaluate the model on unseen data (e.g., 20%).)\n",
    "\n",
    "æ¯”å¦‚é¢„æµ‹è€ƒè¯•æˆç»©ï¼Œç”¨80%å­¦ç”Ÿæ•°æ®è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½é¢„æµ‹æ–°å­¦ç”Ÿçš„æˆç»©ã€‚ (For example, predict exam scores using 80% of student data for training and 20% for testing to check if the model predicts new studentsâ€™ scores.)\n",
    "\n",
    "**äº¤å‰éªŒè¯ï¼ˆK-Foldï¼‰ Cross-Validation (K-Fold)**ï¼š\n",
    "KæŠ˜äº¤å‰éªŒè¯å°†æ•°æ®åˆ†æˆKä»½ï¼Œè®­ç»ƒç”¨K-1ä»½ï¼Œæµ‹è¯•ç”¨1ä»½ï¼Œé‡å¤Kæ¬¡ã€‚ (K-fold cross-validation divides data into K parts, trains on K-1 parts, tests on 1 part, and repeats K times.) å¦‚æœK=5ï¼Œå°†æ•°æ®åˆ†æˆ5ä»½ï¼Œæ¯æ¬¡ç”¨4ä»½è®­ç»ƒï¼Œ1ä»½æµ‹è¯•ï¼Œé‡å¤5æ¬¡ï¼Œæ¯æ¬¡ç”¨ä¸åŒä»½æµ‹è¯•ï¼Œå¹³å‡ç»“æœä»¥è·å¾—å¯é æ€§èƒ½ä¼°è®¡ã€‚ (If K=5, divide data into 5 parts, each time train on 4 parts, test on 1, repeat 5 times with different test parts, and average results for a reliable performance estimate.) ç‰¹åˆ«é€‚åˆå°æ•°æ®é›†ï¼Œæœ€å¤§åŒ–æ•°æ®ä½¿ç”¨ï¼ŒåŒæ—¶é¿å…è¿‡æ‹Ÿåˆã€‚ (Especially suitable for small datasets, maximizing data use while avoiding overfitting.) æ¯”å¦‚100ä¸ªæˆ¿ä»·æ•°æ®ï¼Œ5æŠ˜äº¤å‰éªŒè¯åˆ†æˆ5ç»„ï¼Œæ¯ç»„20ä¸ªã€‚ (For example, 100 house prices, 5-fold CV divides into 5 groups of 20.) æ¯æ¬¡ç”¨80ä¸ªè®­ç»ƒï¼Œ20ä¸ªæµ‹è¯•ï¼Œé‡å¤5æ¬¡ï¼Œå¹³å‡è¯¯å·®ä»¥è¯„ä¼°æ¨¡å‹ã€‚ (Each time train on 80, test on 20, repeat 5 times, and average errors to evaluate the model.)\n",
    "\n",
    "**æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ Time Series Cross-Validation**ï¼š\n",
    "å¯¹æ—¶é—´åºåˆ—æˆ–éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-i.i.d.ï¼‰æ•°æ®ï¼Œä½¿ç”¨TimeSeriesSplitæŒ‰æ—¶é—´é¡ºåºæ‹†åˆ†æ•°æ®ï¼Œé¿å…éšæœºæ‹†åˆ†ã€‚ (For time series or non-independent and identically distributed (non-i.i.d.) data, use TimeSeriesSplit to split data chronologically, avoiding random splitting.) æ—¶é—´åºåˆ—æ•°æ®ï¼ˆä¾‹å¦‚è‚¡ä»·ï¼‰æœ‰æ—¶é—´ä¾èµ–æ€§ï¼Œéšæœºæ‹†åˆ†ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ï¼ˆç”¨æœªæ¥æ•°æ®é¢„æµ‹è¿‡å»ï¼‰ã€‚ (Time series data (e.g., stock prices) has temporal dependencies; random splitting causes data leakage (using future data to predict the past).) TimeSeriesSplitæŒ‰å›ºå®šæ—¶é—´é—´éš”æ‹†åˆ†ï¼ˆä¾‹å¦‚ç”¨ç¬¬1â€“80å¤©è®­ç»ƒï¼Œç¬¬81â€“100å¤©æµ‹è¯•ï¼Œç„¶åç”¨ç¬¬1â€“100å¤©è®­ç»ƒï¼Œç¬¬101â€“120å¤©æµ‹è¯•ï¼‰ï¼Œç¡®ä¿æ—¶é—´ä¾èµ–æ•°æ®çš„çœŸå®é¢„æµ‹ï¼Œé˜²æ­¢è¯¯å¯¼æ€§å¥½ç»“æœã€‚ (TimeSeriesSplit splits by fixed time intervals (e.g., train on days 1â€“80, test on days 81â€“100, then train on days 1â€“100, test on days 101â€“120), ensuring realistic predictions for time-dependent data, preventing misleadingly good results.)\n",
    "\n",
    "## 6.6 å¸¸è§å›å½’æ¨¡å‹å¯¹æ¯”ï¼ˆæ€»ç»“ï¼‰ Comparison of Common Regression Models (Summary)\n",
    "\n",
    "| æ¨¡å‹ç±»å‹ Model Type | ä¼˜åŠ¿ Advantages | é€‚ç”¨åœºæ™¯ Applicable Scenarios |\n",
    "|----------------|-----------|------------|\n",
    "| **çº¿æ€§å›å½’ Linear Regression** | ç®€å•æ˜“è§£é‡Š (Simple and interpretable) | çº¿æ€§å…³ç³»æ˜æ˜¾ï¼Œæ•°æ®è´¨é‡é«˜ (Clear linear relationships, high-quality data) |\n",
    "| **Ridge / Lasso** | å¤„ç†ç‰¹å¾ç›¸å…³æ€§ï¼ŒRidgeç¼©å°ç›¸å…³ç‰¹å¾å½±å“ï¼ŒLassoå°†ä¸é‡è¦ç‰¹å¾ç½®é›¶ã€‚ (Handle feature correlation, Ridge shrinks correlated feature impacts, Lasso sets unimportant features to zero.) æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ (Regularization prevents overfitting) | å¤šç‰¹å¾ã€å¯èƒ½ç›¸å…³ (Multiple features, possible correlations) |\n",
    "| **éšæœºæ£®æ— Random Forest** | é€‚åˆéçº¿æ€§æ•°æ®ï¼Œç»“åˆå¤šä¸ªå†³ç­–æ ‘ã€‚ (Suitable for non-linear data, combines multiple decision trees.) æŠ—è¿‡æ‹Ÿåˆ (Robust against overfitting) | ç‰¹å¾ä¹‹é—´å¤æ‚å…³ç³» (Complex relationships between features) |\n",
    "| **SVMå›å½’ SVM Regression** | é€‚åˆé«˜ç»´ã€éçº¿æ€§æ•°æ® (Suitable for high-dimensional, non-linear data) | ç‰¹å¾ç»´åº¦è¿œå¤§äºæ ·æœ¬æ•° (Feature dimensions far exceed sample size) |\n",
    "| **Logisticå›å½’ Logistic Regression** | å¸¸ç”¨äºåˆ†ç±»ï¼Œä½†ä¹Ÿå¯ç”¨äºå›å½’ï¼Œé¢„æµ‹äº‹ä»¶æ¦‚ç‡ (Commonly used for classification but can be used for regression, predicting event probabilities) | äºŒåˆ†ç±»/æ¦‚ç‡è¾“å‡ºé—®é¢˜ (Binary classification/probability output problems) |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
